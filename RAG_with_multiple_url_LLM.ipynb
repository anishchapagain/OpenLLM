{
  "cells": [
    {
      "metadata": {
        "id": "a938bfee348c0efb"
      },
      "cell_type": "markdown",
      "source": [
        "## RAG: loading multiple URLs and bs4\n"
      ],
      "id": "a938bfee348c0efb"
    },
    {
      "cell_type": "code",
      "id": "f884314f-870c-4bfb-b6c1-a5b4801ec172",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T08:52:46.231582Z",
          "start_time": "2024-10-17T08:52:46.227286Z"
        },
        "id": "f884314f-870c-4bfb-b6c1-a5b4801ec172"
      },
      "source": [
        "import bs4\n",
        "from IPython.core.display import Markdown\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from rich.jupyter import display"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "979c0396812fe56a"
      },
      "cell_type": "markdown",
      "source": [
        "# Web Documents"
      ],
      "id": "979c0396812fe56a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T08:52:47.579812Z",
          "start_time": "2024-10-17T08:52:47.575810Z"
        },
        "id": "3872c9fdc267df8f"
      },
      "cell_type": "code",
      "source": [
        "urls = ['https://research.aimultiple.com/retrieval-augmented-generation/', 'https://kbourne.github.io/chapter1.html']"
      ],
      "id": "3872c9fdc267df8f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "477537d36c3de57d"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Documents\n",
        "### Target specific class from documents"
      ],
      "id": "477537d36c3de57d"
    },
    {
      "cell_type": "code",
      "id": "98ccda2c-0f4c-41c5-804d-2227cdf35aa7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:21.452690Z",
          "start_time": "2024-10-17T09:39:21.446131Z"
        },
        "id": "98ccda2c-0f4c-41c5-804d-2227cdf35aa7"
      },
      "source": [
        "strainer = bs4.SoupStrainer(class_=[\"article_articleDetail__dMzTY\", \"post-single\"])\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=urls,\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=strainer\n",
        "    ),\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:22.921855Z",
          "start_time": "2024-10-17T09:39:22.918495Z"
        },
        "id": "559e51a693a3cea1",
        "outputId": "6dabb53f-cf68-4272-faf1-cb914b312e40"
      },
      "cell_type": "code",
      "source": [
        "loader.web_paths"
      ],
      "id": "559e51a693a3cea1",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['https://research.aimultiple.com/retrieval-augmented-generation/',\n",
              " 'https://kbourne.github.io/chapter1.html']"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:25.756570Z",
          "start_time": "2024-10-17T09:39:24.458484Z"
        },
        "id": "897beb4e35ffc51d"
      },
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "id": "897beb4e35ffc51d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:39.910416Z",
          "start_time": "2024-10-17T09:39:39.906185Z"
        },
        "id": "7ea010485b6e0e35",
        "outputId": "321228fd-0db6-4531-f8d6-2a1babba6d7c"
      },
      "cell_type": "code",
      "source": [
        "for _ in docs:\n",
        "    print(f\"Metadata: {_.metadata}\")\n",
        "    print(f\"Content: {_.page_content[:100].strip()}\")\n",
        "    print(\"\\n\")"
      ],
      "id": "7ea010485b6e0e35",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata: {'source': 'https://research.aimultiple.com/retrieval-augmented-generation/'}\n",
            "Content: Generative AI statsÂ show that Gen AI tools and models like ChatGPT have the potential to automate kn\n",
            "\n",
            "\n",
            "Metadata: {'source': 'https://kbourne.github.io/chapter1.html'}\n",
            "Content: Introduction to Retrieval Augmented Generation (RAG)\n",
            "    \n",
            "Date: March 10, 2024  |  Estimate\n",
            "\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:40.025723Z",
          "start_time": "2024-10-17T09:39:40.022095Z"
        },
        "id": "8d5610ed1fbf203e",
        "outputId": "528dd2e1-36c7-449a-9edc-88a02c80220c"
      },
      "cell_type": "code",
      "source": [
        "'70%' in docs[0].page_content"
      ],
      "id": "8d5610ed1fbf203e",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fcb06b7e44c8c4ee"
      },
      "cell_type": "markdown",
      "source": [
        "# Embedding"
      ],
      "id": "fcb06b7e44c8c4ee"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:40.152971Z",
          "start_time": "2024-10-17T09:39:40.149341Z"
        },
        "id": "780d1c38ffa787f4"
      },
      "cell_type": "code",
      "source": [
        "embeddingOllama = OllamaEmbeddings(\n",
        "    model='nomic-embed-text',\n",
        "    show_progress=False,\n",
        "    # persist_directory=persist_directory\n",
        ")"
      ],
      "id": "780d1c38ffa787f4",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1152ecb5d2b24f8b"
      },
      "cell_type": "markdown",
      "source": [
        "# Split texts"
      ],
      "id": "1152ecb5d2b24f8b"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:40.163372Z",
          "start_time": "2024-10-17T09:39:40.160642Z"
        },
        "id": "9d85364e069fee8d"
      },
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "id": "9d85364e069fee8d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:40.173799Z",
          "start_time": "2024-10-17T09:39:40.170227Z"
        },
        "id": "a4aa213db45a7b3b"
      },
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2024,\n",
        "    chunk_overlap=200\n",
        ")"
      ],
      "id": "a4aa213db45a7b3b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:40.184685Z",
          "start_time": "2024-10-17T09:39:40.180499Z"
        },
        "id": "927a4c65-aa05-486c-8295-2f99673e7c20"
      },
      "cell_type": "code",
      "source": [
        "# Split\n",
        "\n",
        "# text_splitter = SemanticChunker(embeddingOllama)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "id": "927a4c65-aa05-486c-8295-2f99673e7c20",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:39:40.193408Z",
          "start_time": "2024-10-17T09:39:40.189416Z"
        },
        "id": "ffa515eef5fd6e39",
        "outputId": "cb315ae6-512a-42a2-b105-1b01a091ce54"
      },
      "cell_type": "code",
      "source": [
        "len(splits)"
      ],
      "id": "ffa515eef5fd6e39",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "61495f2c92e0652b"
      },
      "cell_type": "markdown",
      "source": [
        "# Vector Database"
      ],
      "id": "61495f2c92e0652b"
    },
    {
      "cell_type": "code",
      "id": "6b13568c-d633-464d-8c43-0d55f34cc8c1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:08.195053Z",
          "start_time": "2024-10-17T09:39:40.206442Z"
        },
        "id": "6b13568c-d633-464d-8c43-0d55f34cc8c1"
      },
      "source": [
        "# Embed\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddingOllama,\n",
        "    persist_directory=None,  # path\n",
        ")\n",
        "\n",
        "# vectorstore.persist() # if 'persist_directory' path is provided\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "374e671514df209a"
      },
      "cell_type": "markdown",
      "source": [
        "## retriever search types"
      ],
      "id": "374e671514df209a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:08.206025Z",
          "start_time": "2024-10-17T09:41:08.202005Z"
        },
        "id": "3d15be3db155dff3",
        "outputId": "4730e172-e97c-4e38-8c35-db5fa0c0cd44"
      },
      "cell_type": "code",
      "source": [
        "retriever.allowed_search_types"
      ],
      "id": "3d15be3db155dff3",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('similarity', 'similarity_score_threshold', 'mmr')"
            ]
          },
          "execution_count": 197,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:08.216156Z",
          "start_time": "2024-10-17T09:41:08.213243Z"
        },
        "id": "5f94e7b4c86fb7a5",
        "outputId": "3ad2dc9e-e26e-4264-f1c7-09e6f04dab9d"
      },
      "cell_type": "code",
      "source": [
        "retriever.search_type"
      ],
      "id": "5f94e7b4c86fb7a5",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'similarity'"
            ]
          },
          "execution_count": 198,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f27cc38ed661cf48"
      },
      "cell_type": "markdown",
      "source": [
        "# Prompt Template"
      ],
      "id": "f27cc38ed661cf48"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:08.246900Z",
          "start_time": "2024-10-17T09:41:08.243602Z"
        },
        "id": "b162cca29bd6569c"
      },
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ],
      "id": "b162cca29bd6569c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:08.265161Z",
          "start_time": "2024-10-17T09:41:08.262058Z"
        },
        "id": "aef1a608b9a521f4"
      },
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template)"
      ],
      "id": "aef1a608b9a521f4",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "e8975479-b3e3-481d-ad7b-08b4eb3faaef",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:08.277287Z",
          "start_time": "2024-10-17T09:41:08.272067Z"
        },
        "id": "e8975479-b3e3-481d-ad7b-08b4eb3faaef"
      },
      "source": [
        "# Post-processing\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "18b40116aff40f7d"
      },
      "cell_type": "markdown",
      "source": [
        "## LLM from Ollama"
      ],
      "id": "18b40116aff40f7d"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:08.286933Z",
          "start_time": "2024-10-17T09:41:08.284640Z"
        },
        "id": "2f3d7f43bfa115f0"
      },
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"llama3.2\",\n",
        "    temperature=0.5\n",
        ")"
      ],
      "id": "2f3d7f43bfa115f0",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f42498d0f3682bcc"
      },
      "cell_type": "markdown",
      "source": [
        "### Chain it all together with LangChain"
      ],
      "id": "f42498d0f3682bcc"
    },
    {
      "cell_type": "code",
      "id": "fd9db713-f705-4b65-800e-2c4e3d0e4ef4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:08.297027Z",
          "start_time": "2024-10-17T09:41:08.294124Z"
        },
        "id": "fd9db713-f705-4b65-800e-2c4e3d0e4ef4"
      },
      "source": [
        "rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "e6adfa9646771294"
      },
      "cell_type": "markdown",
      "source": [
        "## QnA Section"
      ],
      "id": "e6adfa9646771294"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:08.306435Z",
          "start_time": "2024-10-17T09:41:08.304186Z"
        },
        "id": "1ef6470ec2a1031c"
      },
      "cell_type": "code",
      "source": [
        "def ask_rag(question: str) -> None:\n",
        "    \"\"\"\n",
        "    Asks a question to the RAG chain and displays the answer in Markdown format.\n",
        "\n",
        "    Parameters:\n",
        "    question (str): The question to be asked.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Invoke the RAG chain with the question\n",
        "    response = rag_chain.invoke(question)\n",
        "\n",
        "    # Display the response in Markdown format\n",
        "    display(Markdown(response))"
      ],
      "id": "1ef6470ec2a1031c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:13.128700Z",
          "start_time": "2024-10-17T09:41:08.313421Z"
        },
        "id": "cb7c151010bd4164",
        "outputId": "7ae24f55-6a8d-4c3e-d265-c6f366737d7d"
      },
      "cell_type": "code",
      "source": [
        "question = ''\n",
        "ask_rag(question)"
      ],
      "id": "cb7c151010bd4164",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I can help you answer questions about Retrieval Augmented Generation (RAG). What is your question?"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:18.417911Z",
          "start_time": "2024-10-17T09:41:13.135362Z"
        },
        "id": "8b30177a-f9ab-45e4-812d-33b0f97325bd",
        "outputId": "2e51abf2-7a32-4318-b27d-ded79905993f"
      },
      "cell_type": "code",
      "source": [
        "# Question - run the chain\n",
        "question = \"What are the advantages of using RAG?\"\n",
        "ask_rag(question)"
      ],
      "id": "8b30177a-f9ab-45e4-812d-33b0f97325bd",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The potential advantages of using RAG include:\n\n1. Improved accuracy and relevance\n2. Customization\n3. Flexibility\n4. Expanding the model's knowledge beyond the training data.\n\nThese advantages are further explored in the context of leveraging LLM (Large Language Model) within a company's private or specific data needs."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:23.959794Z",
          "start_time": "2024-10-17T09:41:18.425554Z"
        },
        "id": "7082f647-bf11-4dee-8121-ae8c8a66cb4b",
        "outputId": "19ca6d9f-c290-43d7-c9b0-fd9eddf5bc49"
      },
      "cell_type": "code",
      "source": [
        "ask_rag(\"What are the disadvantages of using RAG?\")"
      ],
      "id": "7082f647-bf11-4dee-8121-ae8c8a66cb4b",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know the disadvantages of using RAG. The retrieved context only discusses its advantages, such as being superior for retrieving factual information that is not present in the LLM's training data or is private, and allowing for dynamic integration of external knowledge without modifying the model's weights. It does not mention any potential drawbacks or disadvantages of using RAG."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:29.236514Z",
          "start_time": "2024-10-17T09:41:23.970453Z"
        },
        "id": "fec4ddb37e5efbe7",
        "outputId": "512212eb-d591-4e3b-a0cb-794c83b03603"
      },
      "cell_type": "code",
      "source": [
        "ask_rag(\"What are the limitations of using RAG?\")"
      ],
      "id": "fec4ddb37e5efbe7",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The limitations of using RAG (Retrieval-Augmented Generation) are not explicitly stated in the provided context. However, I can infer that it is generally superior to fine-tuning for retrieving factual information that is not present in the LLM's training data or is private.\n\nI don't know the specific limitations of RAG beyond what is mentioned in the context."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:36.119263Z",
          "start_time": "2024-10-17T09:41:29.243667Z"
        },
        "id": "466c3f29e6da519a",
        "outputId": "d52c7f72-e971-45d9-94f1-2d174c35995a"
      },
      "cell_type": "code",
      "source": [
        "ask_rag(\"Compare RAG with GenerativeAI\")"
      ],
      "id": "466c3f29e6da519a",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided context, I can compare RAG (Relevance-Aware Generation) with GenerativeAI.\n\nWhile the text doesn't provide direct information about GenerativeAI's capabilities or methodology, we can infer some differences between RAG and GenerativeAI:\n\n1. Data usage: RAG utilizes internal company data to improve its performance, whereas GenerativeAI seems to rely on external knowledge bases or training data.\n2. Approach: RAG is a hybrid approach that combines retrieval-augmented generation with a specialist LM drafter and a generalist LM verifier. In contrast, the text suggests that conventional GenerativeAI lacks this level of internal understanding and data integration.\n3. Goals: RAG aims to enhance factual recall and refine performance on specialized tasks by leveraging internal company data. GenerativeAI's goals are not explicitly stated in the context.\n\nHowever, I don't know enough about GenerativeAI's specific capabilities, strengths, or weaknesses to make a more detailed comparison."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:42.401491Z",
          "start_time": "2024-10-17T09:41:36.124606Z"
        },
        "id": "7f26638fbfb53961",
        "outputId": "b9c90483-260c-423a-e9b4-3a42bc1140b2"
      },
      "cell_type": "code",
      "source": [
        "question = \"Explain in brief RAG and fine-tuning. Also mention the source url from where the answer is being collected\"\n",
        "ask_rag(question)"
      ],
      "id": "7f26638fbfb53961",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided context, here's a brief explanation of RAG and fine-tuning:\n\nRAG (Relational Attention-based Graph) is generally superior for retrieving factual information that is not present in the LLM's training data or is private. It allows for dynamic integration of external knowledge without modifying the model's weights.\n\nFine-tuning, on the other hand, is more suitable for teaching the model specialized tasks or adapting it to a specific domain. However, it requires careful consideration of context window sizes and the potential for overfitting when fine-tuning on a specific dataset.\n\nSource URL: Unfortunately, I couldn't find a specific source URL in the provided context, but the information seems to be based on general knowledge about RAG and fine-tuning in natural language processing (NLP) tasks."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:49.236125Z",
          "start_time": "2024-10-17T09:41:42.412627Z"
        },
        "id": "3ed5cc9a334781ee",
        "outputId": "ebf3c504-c737-4029-a7ff-7537266332bc"
      },
      "cell_type": "code",
      "source": [
        "question = \"Are there any other retrieval models like RAG?\"\n",
        "ask_rag(question)"
      ],
      "id": "3ed5cc9a334781ee",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the context provided, it appears that Retrieval-augmented Generation (RAG) is a hybrid approach that combines elements of both retrieval and generation models.\n\nAs for other retrieval models like RAG, I'm not aware of any specific ones mentioned in the context. However, some other retrieval models mentioned are:\n\n1. BART with Retrieval: This seems to be a variant of the BART model that incorporates retrieval capabilities.\n2. BM25: A widely used retrieval algorithm for text search.\n3. ColBERT Model: A contextualized word embedding model that can be used for retrieval tasks.\n4. DPR (Document Passage Retrieval) Model: A state-of-the-art retrieval model that can be fine-tuned for specific tasks.\n\nIt's worth noting that RAG is a relatively new and emerging approach, and there may not be many other retrieval models specifically designed to work in a similar way."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:41:55.223786Z",
          "start_time": "2024-10-17T09:41:49.242427Z"
        },
        "id": "a8ad90eb1c4da327",
        "outputId": "265b6ca6-96a1-44e9-9d87-6bffc7e5aa61"
      },
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"Are there any other retrieval models like RAG?\"\n",
        "ask_rag(question)"
      ],
      "id": "a8ad90eb1c4da327",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided context, it appears that Retrieval-Augmented Generation (RAG) is a hybrid approach that combines elements of both retrieval and generation models to improve the quality and relevance of generated content.\n\nAs for other retrieval models like RAG, I couldn't find any specific information in the context. However, the context does mention some other retrieval models such as BM25, ColBERT Model, and DPR (Document Passage Retrieval) Model, but it doesn't explicitly state that they are similar to RAG.\n\nIf you're looking for alternative retrieval models that share similarities with RAG, I would suggest exploring research papers or academic articles on the topic of hybrid retrieval-augmented generation models."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:42:00.739903Z",
          "start_time": "2024-10-17T09:41:55.230609Z"
        },
        "id": "f62d4b27d3d3e7c9",
        "outputId": "5f643400-2639-4a05-f9df-06489d026781"
      },
      "cell_type": "code",
      "source": [
        "ask_rag(\"provide me with all the links available\")"
      ],
      "id": "f62d4b27d3d3e7c9",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know. The provided context doesn't mention any specific links. It appears to be a discussion about question-answering tasks and the strengths and weaknesses of different approaches, including Retrieval-Augmented Generative Models (RAGs) and other techniques. If you could provide more context or clarify what you're looking for, I'd be happy to try and help further."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-17T09:42:05.571215Z",
          "start_time": "2024-10-17T09:42:00.753095Z"
        },
        "id": "d4636cf5a62f1bc4",
        "outputId": "fed40512-43ba-498c-e3e3-63931eae9cc6"
      },
      "cell_type": "code",
      "source": [
        "ask_rag(\"what is estimated reading time?\")"
      ],
      "id": "d4636cf5a62f1bc4",
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know the answer to that question. The context provided only discusses Retrieval Augmented Generation (RAG) and its potential for information overload, but does not mention estimated reading time."
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}